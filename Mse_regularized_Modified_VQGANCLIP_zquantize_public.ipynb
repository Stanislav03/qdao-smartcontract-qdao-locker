{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mse regularized Modified_VQGANCLIP_zquantize public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stanislav03/qdao-smartcontract-qdao-locker/blob/master/Mse_regularized_Modified_VQGANCLIP_zquantize_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# Generates images from text prompts with VQGAN and CLIP (z+quantize method).\n",
        "\n",
        "By https://twitter.com/jbusted1 .\n",
        "Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
        "\n",
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=mse_regulizer_colab_vqgan)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkUfzT60ZZ9q",
        "outputId": "b9e7a81a-3778-4dfc-dc15-6eac52d2a7b7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 26 18:20:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wSfISAhyPmyp",
        "outputId": "1b6434a9-023f-4247-b294-e977e65d7ce5"
      },
      "source": [
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 185 (delta 7), reused 14 (delta 4), pack-reused 168\u001b[K\n",
            "Receiving objects: 100% (185/185), 8.90 MiB | 27.37 MiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1297, done.\u001b[K\n",
            "remote: Counting objects: 100% (487/487), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 1297 (delta 32), reused 459 (delta 19), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1297/1297), 409.22 MiB | 33.34 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "Checking out files: 100% (510/510), done.\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.5.7-py3-none-any.whl (526 kB)\n",
            "\u001b[K     |████████████████████████████████| 526 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 74.4 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.10.0.2)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.10.0+cu111)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.7.0)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 79.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.8)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 66.4 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Building wheels for collected packages: ftfy, antlr4-python3-runtime, future\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=d9250748a429e69aef5bcaf8491045192cbe97f61556419a5b4c1f187a96761d\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=ad3b243e019f5943ee42fb538f90bff912d3771c345d1b9851393ae304ffcca4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=9c0bd1b10525130bad8498e4307992aed860a6cfd8901421316c190504c62c4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built ftfy antlr4-python3-runtime future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.2.0 fsspec-2021.11.1 ftfy-6.0.3 future-0.18.2 multidict-5.2.0 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.5.7 torchmetrics-0.6.2 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.6.2-py2.py3-none-any.whl (401 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 34.6 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 184 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 204 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 215 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 235 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 245 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 256 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 266 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 286 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 296 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 307 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 317 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 327 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 337 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 348 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 358 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 368 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 378 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 399 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 401 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.6)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.2\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhhdWrSxQhwg",
        "outputId": "2ed23e77-611c-428f-96a0-026417789c72"
      },
      "source": [
        "\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\n",
        "\n",
        "# Mirrors\n",
        "# !curl -L 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.yaml' > vqgan_imagenet_f16_16384.yaml\n",
        "# !curl -L 'http://mirror.io.community/blob/vqgan/vqgan_imagenet_f16_16384.ckpt' > vqgan_imagenet_f16_16384.ckpt \n",
        "\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.ckpt > vqgan_openimages_f16_8192.ckpt\n",
        "# !curl -L https://dl.nmkd.de/ai/clip/vqgan/8k-2021-06/vqgan-f8-8192.yaml > vqgan_openimages_f16_8192.yaml\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   692  100   692    0     0    791      0 --:--:-- --:--:-- --:--:--   791\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  934M  100  934M    0     0  14.2M      0  0:01:05  0:01:05 --:--:-- 14.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMSuW2EQWsd"
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvnTBhPGT1gn"
      },
      "source": [
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        # if args.use_augs:\n",
        "        #   cutouts = augs(cutouts)\n",
        "\n",
        "        # if args.noise_fac:\n",
        "        #   facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "        #   cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "%mkdir /content/vids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "# ARGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLw9p5Rzacso"
      },
      "source": [
        "args = argparse.Namespace(\n",
        "    \n",
        "    prompts=[\"a photo-realistic and beautiful painting of an old man sitting on a chair next to a giant iceberg and looking at a green lush landscape.\"],\n",
        "    size=[640, 512], \n",
        "    init_image= None,\n",
        "    init_weight= 0.5,\n",
        "\n",
        "    # clip model settings\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config='vqgan_imagenet_f16_16384.yaml',         \n",
        "    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt',\n",
        "    step_size=0.1,\n",
        "    \n",
        "    # cutouts / crops\n",
        "    cutn=64,\n",
        "    cut_pow=1,\n",
        "\n",
        "    # display\n",
        "    display_freq=25,\n",
        "    seed=158758,\n",
        "    use_augs = True,\n",
        "    noise_fac= 0.1,\n",
        "    ema_val = 0.99,\n",
        "\n",
        "    record_generation=True,\n",
        "\n",
        "    # noise and other constraints\n",
        "    use_noise = None,\n",
        "    constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "    # add noise to embedding\n",
        "    noise_prompt_weights = None,\n",
        "    noise_prompt_seeds = [14575],#\n",
        "\n",
        "    # mse settings\n",
        "    mse_withzeros = True,\n",
        "    mse_decay_rate = 50,\n",
        "    mse_epoches = 5,\n",
        "    mse_quantize = True,\n",
        "\n",
        "    # end itteration\n",
        "    max_itter = -1,\n",
        ")\n",
        "\n",
        "mse_decay = 0\n",
        "if args.init_weight:\n",
        "  mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# <AUGMENTATIONS>\n",
        "augs = nn.Sequential(\n",
        "    \n",
        "    K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "    )\n",
        "\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "image.save('init3.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crRQdV3jPSvw"
      },
      "source": [
        "# Constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD9R97ygRN-0"
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "if args.constraint_regions and args.init_image:\n",
        "  \n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  toksX, toksY = args.size[0] // 16, args.size[1] // 16\n",
        "\n",
        "  pil_image = Image.open(args.init_image).convert('RGB')\n",
        "  pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "\n",
        "  width, height = pil_image.size\n",
        "\n",
        "  d = ImageDraw.Draw(pil_image)\n",
        "  for i in range(0,width,16):\n",
        "      d.text((i+4,0), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "  for i in range(0,height,16):\n",
        "      d.text((4,i), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "\n",
        "  pil_image = TF.to_tensor(pil_image)\n",
        "\n",
        "  print(pil_image.shape)\n",
        "  for i in range(pil_image.shape[1]):\n",
        "    for j in range(pil_image.shape[2]):\n",
        "      if i%16 == 0 or j%16 ==0:\n",
        "        pil_image[:,i,j] = 0\n",
        "\n",
        "  # select region\n",
        "  c_h = [16,32]\n",
        "  c_w = [0,40]\n",
        "\n",
        "  c_hf = [i*16 for i in c_h]\n",
        "  c_wf = [i*16 for i in c_w]\n",
        "\n",
        "  pil_image[0,c_hf[0]:c_hf[1],c_wf[0]:c_wf[1]] = 0\n",
        "\n",
        "  TF.to_pil_image(pil_image.cpu()).save('progress.png')\n",
        "  display.display(display.Image('progress.png'))\n",
        "\n",
        "  z_mask = torch.zeros([1, 256, int(height/16), int(width/16)]).to(device)\n",
        "  z_mask[:,:,c_h[0]:c_h[1],c_w[0]:c_w[1]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgTa_JWi7Sn"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7EDme5RYCrt"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "print('using prompts: ', args.prompts)\n",
        "\n",
        "tv_loss = TVLoss() \n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "mse_weight = args.init_weight\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "# e_dim = model.quantize.e_dim\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "if args.seed is not None:\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "    pil_image = TF.to_tensor(pil_image)\n",
        "    if args.use_noise:\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "else:\n",
        "    \n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "z = EMATensor(z, args.ema_val)\n",
        "\n",
        "if args.mse_withzeros and not args.init_image:\n",
        "  z_orig = torch.zeros_like(z.tensor)\n",
        "else:\n",
        "  z_orig = z.tensor.clone()\n",
        "\n",
        "\n",
        "opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "    # pMs[0].embed = pMs[0].embed + Prompt(embed, weight, stop).embed.to(device)\n",
        "\n",
        "\n",
        "def synth(z, quantize=True):\n",
        "    if args.constraint_regions:\n",
        "      z = replace_grad(z, z * z_mask)\n",
        "\n",
        "    if quantize:\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "      else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "\n",
        "    else:\n",
        "      z_q = z.model\n",
        "\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z.average, True)\n",
        "\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')   \n",
        "    display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "def ascend_txt():\n",
        "    global mse_weight\n",
        "\n",
        "    out = synth(z.tensor)\n",
        "\n",
        "    if args.record_generation:\n",
        "      with torch.no_grad():\n",
        "        global vid_index\n",
        "        out_a = synth(z.average, True)\n",
        "        TF.to_pil_image(out_a[0].cpu()).save(f'/content/vids/{vid_index}.png')\n",
        "        vid_index += 1\n",
        "\n",
        "    cutouts = make_cutouts(out)\n",
        "\n",
        "    if args.use_augs:\n",
        "      cutouts = augs(cutouts)\n",
        "\n",
        "    if args.noise_fac:\n",
        "      facs = cutouts.new_empty([args.cutn, 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "      cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        \n",
        "        global z_orig\n",
        "        \n",
        "        result.append(F.mse_loss(z.tensor, z_orig) * mse_weight / 2)\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "            if args.mse_quantize:\n",
        "              z_orig = vector_quantize(z.average.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
        "            else:\n",
        "              z_orig = z.average.clone()\n",
        "\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "              mse_weight = mse_weight - mse_decay\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "            else:\n",
        "              mse_weight = 0\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "vid_index = 0\n",
        "def train(i):\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    z.update()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True and i != args.max_itter:\n",
        "\n",
        "            train(i)\n",
        "\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "              z = EMATensor(z.average, args.ema_val)\n",
        "              opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# create video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT3hKb5gJUPq",
        "outputId": "b8fed573-0435-4a71-b3d3-3da889b93e47"
      },
      "source": [
        "%cd vids\n",
        "\n",
        "images = \"%d.png\"\n",
        "video = \"/content/old_man_iceberg.mp4\"\n",
        "!ffmpeg -r 30 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\n",
        "\n",
        "%cd ..\n",
        "\n",
        "from google.colab import files\n",
        "files.download('old_man_iceberg.mp4') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/vids\n",
            "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
            "  libavutil      55. 78.100 / 55. 78.100\n",
            "  libavcodec     57.107.100 / 57.107.100\n",
            "  libavformat    57. 83.100 / 57. 83.100\n",
            "  libavdevice    57. 10.100 / 57. 10.100\n",
            "  libavfilter     6.107.100 /  6.107.100\n",
            "  libavresample   3.  7.  0 /  3.  7.  0\n",
            "  libswscale      4.  8.100 /  4.  8.100\n",
            "  libswresample   2.  9.100 /  2.  9.100\n",
            "  libpostproc    54.  7.100 / 54.  7.100\n",
            "Input #0, image2, from '%d.png':\n",
            "  Duration: 00:00:20.48, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 640x512, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mprofile High, level 3.0\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=20.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to '/content/old_man_iceberg.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf57.83.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 640x512, q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc57.107.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "\u001b[0;36m[png @ 0x55a2a3c64300] \u001b[0m\u001b[1;31m0 bytes left\n",
            "\u001b[0m\u001b[1;31mError while decoding stream #0:0: Invalid data found when processing input\n",
            "frame=  511 fps= 34 q=-1.0 Lsize=    4722kB time=00:00:16.93 bitrate=2284.4kbits/s speed=1.12x    \n",
            "video:4715kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.145122%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mframe I:5     Avg QP:18.80  size: 35975\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mframe P:132   Avg QP:20.98  size: 26275\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mframe B:374   Avg QP:24.24  size:  3154\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mconsecutive B-frames:  2.2%  0.4%  1.2% 96.3%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mmb I  I16..4:  9.1% 68.1% 22.7%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mmb P  I16..4:  0.8%  5.5%  4.7%  P16..4: 42.6% 24.2% 15.0%  0.0%  0.0%    skip: 7.3%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mmb B  I16..4:  0.0%  0.1%  0.1%  B16..8: 32.6%  3.3%  1.7%  direct: 5.7%  skip:56.6%  L0:42.8% L1:50.6% BI: 6.6%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0m8x8 transform intra:54.2% inter:51.1%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mcoded y,uvDC,uvAC intra: 91.0% 90.2% 68.5% inter: 24.7% 28.6% 3.3%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mi16 v,h,dc,p: 26% 31%  6% 37%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 31% 17%  5%  4%  4%  6%  5% 10%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 12% 29% 14%  7%  6%  5% 10%  6% 12%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mi8c dc,h,v,p: 44% 34% 14%  8%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mWeighted P-Frames: Y:24.2% UV:17.4%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mref P L0: 64.4% 17.7% 12.6%  4.6%  0.8%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mref B L0: 92.1%  6.4%  1.5%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mref B L1: 96.6%  3.4%\n",
            "\u001b[1;36m[libx264 @ 0x55a2a3c63e00] \u001b[0mkb/s:2267.39\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiZMW3kAUD1f"
      },
      "source": [
        "delete all frames from folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsixT6gqJ8aY",
        "outputId": "31aa3776-b614-4f48-e2d1-df7cfb69a31e"
      },
      "source": [
        "%cd vids\n",
        "%rm *.png\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/vids\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}